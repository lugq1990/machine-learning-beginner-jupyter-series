{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('tensorflow2': conda)"
  },
  "interpreter": {
   "hash": "9eb97d63258c50a4da8652d2383f96624412435e33e64bdb86e28fe9564bf9d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Adaboost algorithm implement\n",
    "\n",
    "Try to implement Adaboost with hands-on tutorial from frist beginning. **Adaboost** is an ensemble algorithm that will use a ensemble of stamp to make prediction and training, the main step for Adaboost is followed as bellow:\n",
    "\n",
    "1. Give each sample with a weight, first time should be: 1/n_sample.\n",
    "2. Build a stump from features that we have based on Gini or Entropy.\n",
    "3. Evaluate model for each class with correct prediction and wrong. \n",
    "4. Compute `amount for say` with 1/2 * log((1 - total_error)/total_error).\n",
    "5. Update each sample with new weights, weight is computed by formula:\n",
    "   For correct prediction sample: \n",
    "                                weight * e(-amount_of_say)\n",
    "   For in-correct prediction sample:\n",
    "                                weight * e(amount_of_say)\n",
    "   Tips:\n",
    "   There is a main idea to update each sample weight based on `how well the stamp does`. For example, for in-correct sample weight update, if current stamp does well, then `amount_of_say` will be reall large, then in-correct weight will be greater than before, if current stamp doesn't do well, then `amount_of_say` will be smaller, so in theory if current stamp doesn't do well for in-correct sample, then current  weights will be bigger, as for correct prediction sample will be smaller.\n",
    "6. Create a new stamp based on weighted Gini Index or we could create a new sample datasets based on the weights with repeated selection of samples for larger weights.\n",
    "7. Step repeated.\n",
    "\n",
    "\n",
    "Start to implement with **Adaboost**.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# random data and label for easy doing\n",
    "data = np.array([[3, 3, 6, 4, 6, 3], [1.2, 2.1, 2.3, 3.4, 5.3, 1.8]]).T\n",
    "label = np.array([1, 0, 1, 1, 1, 0])\n",
    "\n",
    "weight = np.array([1/len(label) for _ in range(len(label))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Root gini:  0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "# First should with Gini index compute\n",
    "def gini_index(y):\n",
    "    \"\"\"compute each class times with 1 - sum(prob**2)\"\"\"\n",
    "    counter = list(Counter(y).values())\n",
    "    probs = [d / len(y) for d in counter]\n",
    "\n",
    "    gini = 1 - sum([prob **2 for prob in probs])\n",
    "\n",
    "    return gini\n",
    "\n",
    "print(\"Root gini: \", gini_index(label))"
   ]
  },
  {
   "source": [
    "### Stamp creation\n",
    "\n",
    "Next step should create a stamp based on each feature with best gini index as a root stamp."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Category columns to process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected value: 6.0 with max gini: 0.05555555555555558 to split.\n"
     ]
    }
   ],
   "source": [
    "# First try 1th column, get unique data and to get best split gini\n",
    "first_data = data[:, 0]\n",
    "unique_values = np.unique(first_data)\n",
    "\n",
    "root_gini = gini_index(label)\n",
    "\n",
    "gini_list = []\n",
    "# loop each columns with equal to that value to get label.\n",
    "for val in unique_values:\n",
    "    # so we could create a binary stamp.\n",
    "    val_left_label = label[first_data == val]\n",
    "    val_right_label = label[first_data != val]\n",
    "\n",
    "    val_left_gini = gini_index(val_left_label)\n",
    "    val_right_gini = gini_index(val_right_label)\n",
    "\n",
    "    val_gini = val_left_gini + val_right_gini - root_gini\n",
    "\n",
    "    gini_list.append(val_gini)\n",
    "\n",
    "# select max gini-index value's index to select root step value\n",
    "max_gini_cat = max(gini_list)\n",
    "select_value_cat = unique_values[np.argmax(gini_list)]\n",
    "print(\"Selected value: {} with max gini: {} to split.\".format(select_value_cat, max_gini_cat))"
   ]
  },
  {
   "source": [
    "Continues data to process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected value: 2.2 with max gini: 0.4444444444444444 to split.\n"
     ]
    }
   ],
   "source": [
    "second_data = data[:, 1]\n",
    "\n",
    "# sort them and get median value\n",
    "second_data = sorted(second_data)\n",
    "median_data = [(second_data[i] + second_data[i+1])/2 for i in range(len(second_data) - 1)]\n",
    "\n",
    "root_gini = gini_index(label)\n",
    "\n",
    "gini_list = []\n",
    "\n",
    "# loop each data with lower and higher to make a binary tree\n",
    "for val in median_data:\n",
    "    val_left_label = label[second_data <=  val]\n",
    "    val_right_label = label[second_data >  val]\n",
    "\n",
    "    val_left_gini = gini_index(val_left_label)\n",
    "    val_right_gini = gini_index(val_right_label)\n",
    "\n",
    "    val_gini = val_left_gini + val_right_gini - root_gini\n",
    "\n",
    "    gini_list.append(val_gini)\n",
    "\n",
    "\n",
    "# select max gini-index value's index to select root step value\n",
    "max_gini_con = max(gini_list)\n",
    "select_value_con = median_data[np.argmax(gini_list)]\n",
    "print(\"Selected value: {} with max gini: {} to split.\".format(select_value_con, max_gini_con))"
   ]
  },
  {
   "source": [
    "Select max gini value as first stamp. So we could get that for now, to select 2th column with median value: 1.95 will get max gini index. So let's select it as root stamp value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 0 0] [1 1 1]\nLeft pred: 0, right pred: 1\n"
     ]
    }
   ],
   "source": [
    "select_col = 1\n",
    "select_val = select_value_con\n",
    "\n",
    "# we need to make prediction! so the prediction is based on how many label in that leaves\n",
    "left_label = label[data[:, select_col] <= select_val]\n",
    "right_label = label[data[:, select_col] > select_val]\n",
    "\n",
    "left_weight = weight[data[:, select_col] <= select_val]\n",
    "right_weight = weight[data[:, select_col] > select_val]\n",
    "\n",
    "print(left_label, right_label)\n",
    "\n",
    "# the prediction is the max number of that leaves. so for now is both of them are predict as 1.\n",
    "left_pred = Counter(left_label).most_common()[0][0]\n",
    "right_pred = Counter(right_label).most_common()[0][0]\n",
    "\n",
    "print(\"Left pred: {}, right pred: {}\".format(left_pred, right_pred))\n",
    "\n",
    "\n",
    "# make this stamp into a function:\n",
    "def stamp_pred(data, select_col=1):\n",
    "    select_val = select_value_con\n",
    "    \n",
    "    if data[select_col] <= select_val:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Compute amount of say with: 1/2*log((1-total_error)/total_error)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This stamp amount of say is : 0.8047189562170503\n"
     ]
    }
   ],
   "source": [
    "# left not equal to 0 should be mis-correct, right not equal to 1 is mis-correct\n",
    "left_error = [x != 0 for x in left_label]\n",
    "right_error = [x != 1 for x in right_label]\n",
    "\n",
    "# total_error should based on the weight, not with number\n",
    "total_error = sum(left_weight[left_error].tolist() + right_weight[right_error].tolist())\n",
    "\n",
    "amount_of_say = 1/2 * np.log((1 - total_error) / total_error)\n",
    "\n",
    "print(\"This stamp amount of say is : {}\".format(amount_of_say))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[False  True  True  True  True  True]\n[0.5 0.1 0.1 0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "# loop each data point to get prediction \n",
    "pred = np.array([stamp_pred(x) for x in data])\n",
    "\n",
    "\n",
    "def weight_update(pred, weight):\n",
    "    # based on correct pred or not.\n",
    "    \n",
    "    # This is used to get different type for correct and not weight update policy.\n",
    "    correct_pred = label == pred\n",
    "    new_weight = np.empty_like(weight)\n",
    "\n",
    "    for i in range(len(weight)):\n",
    "        if correct_pred[i]:\n",
    "            # correct:\n",
    "            new_w = weight[i] * np.exp(-amount_of_say)\n",
    "        else:\n",
    "            new_w = weight[i] * np.exp(amount_of_say)\n",
    "        new_weight[i] = new_w\n",
    "\n",
    "    # Normalize new_weight \n",
    "    new_weight /= np.sum(new_weight)\n",
    "\n",
    "    return new_weight, correct_pred\n",
    "\n",
    "new_weight, correct_or_not = weight_update(pred, weight)\n",
    "\n",
    "# So that we do see that for correct pred's weight is smaller and miss-correct's weight is larger.\n",
    "print(correct_or_not)\n",
    "print(new_weight)\n"
   ]
  },
  {
   "source": [
    "So we could try to build a new dataset based on these weights. Let's try to make a random value and get the first satisfied value index, and add it into new dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Get new dataset\n[[3.  1.2]\n [3.  1.2]\n [3.  1.2]\n [3.  1.2]\n [3.  1.2]\n [3.  2.1]]\n"
     ]
    }
   ],
   "source": [
    "cumsum = np.cumsum(new_weight)\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    r = np.random.random()\n",
    "    select_index = (r < cumsum).tolist().index(True)\n",
    "\n",
    "    new_dataset.append(data[select_index])\n",
    "\n",
    "new_dataset = np.array(new_dataset)\n",
    "\n",
    "print(\"Get new dataset\")\n",
    "print(new_dataset)"
   ]
  },
  {
   "source": [
    "Then we could repeat these steps to build another stamp, but one more thing is that each tree will have a attribute with **amount_of_say** to demonstrate importance of each tree. To make prediction, like we have 10 stamps, then we will get different pred, we could use **voting** to get prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}